{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#sobre-este-projeto","title":"\ud83d\udc68\u200d\ud83d\udcbb Sobre este projeto","text":"<p>Ol\u00e1! Muito prazer, sou o Vinicius! Seja bem-vindo ao meu espa\u00e7o de aprendizado. \ud83d\ude80</p> <p>At\u00e9 o final de 2024, atuei em uma squad de neg\u00f3cios, focado em an\u00e1lise de dados. Por\u00e9m, descobri meu interesse em unir a engenharia de software com dados e estou iniciando minha carreira como engenheiro de dados!</p> <p>Objetivo</p> <p>O objetivo deste site \u00e9 documentar todo o meu processo de aprendizado para me tornar um engenheiro de dados!</p> <p>De acordo com o Microsoft Learn, engenheiros de dados integram, transformam e consolidam dados de sistemas estruturados e n\u00e3o estruturados, criando bases s\u00f3lidas para solu\u00e7\u00f5es de an\u00e1lise. </p>"},{"location":"#objetivos-de-aprendizagem","title":"\ud83c\udfaf Objetivos de Aprendizagem","text":"<p>Ao longo deste estudo, pretendo dominar:</p> <p>\ud83d\udd39Ingest\u00e3o e Processamento de Dados</p> <ul> <li>Implementar ingest\u00e3o de dados com AWS Lambda e Amazon Kinesis.</li> <li>Configurar alertas em tempo real com Amazon SNS.</li> <li>Gerenciar seguran\u00e7a e controle de acesso com AWS IAM.</li> </ul> <p>\ud83d\udd39Armazenamento e Consulta</p> <ul> <li>Armazenar dados de forma escal\u00e1vel no Amazon S3.</li> <li>Automatizar processos ETL com AWS Glue.</li> <li>Realizar consultas eficientes com Amazon Athena.</li> </ul> <p>\ud83d\udd39Monitoramento e Gest\u00e3o</p> <ul> <li>Monitorar pipelines de dados com Amazon CloudWatch.</li> <li>Integrar servi\u00e7os da AWS para criar solu\u00e7\u00f5es de an\u00e1lise escal\u00e1veis.</li> </ul> <p>\ud83d\udd39Design e Infraestrutura</p> <ul> <li>Desenhar arquiteturas de dados eficientes.</li> <li>Implementar Infraestrutura como C\u00f3digo (IaC) com Terraform.</li> <li>Adaptar scripts Terraform em uma pipeline CI/CD no GitHub Actions.</li> </ul>"},{"location":"00-fundamentos-de-dados/","title":"Fundamentos de Dados","text":"<p>Antes de iniciar os estudos pr\u00e1ticos em engenharia de dados, como a democratiza\u00e7\u00e3o de dados com AWS Glue e Spark, \u00e9 fundamental compreender os conceitos essenciais da \u00e1rea. Esta se\u00e7\u00e3o apresentar\u00e1 os fundamentos necess\u00e1rios para uma base s\u00f3lida no tema.</p>"},{"location":"00-fundamentos-de-dados/#1-o-que-e-big-data","title":"1. O que \u00e9 Big Data?","text":"<p>De acordo com o site da Oracle, Big Data \u00e9 um conjunto de dados muito grandes, com uma variedade de tipos e fontes. At\u00e9 o momento de escrita desse reposit\u00f3rio, os princ\u00edpios do Big Data s\u00e3o baseados em 5 V's:</p> <ul> <li>Volume: as organiza\u00e7\u00f5es coletam dados de v\u00e1rias fontes, tornando o volume muito grande.</li> <li>Velocidade: os dados s\u00e3o transmitidos em alta velocidade e devem ser tratados em tempo h\u00e1bil.</li> <li>Variedade: os dados v\u00eam em todos os tipos e formatos. Dados estruturados (padronizados, com os mesmos atributos para todos os valores de dados). Semi estruturados (json). N\u00e3o Estruturados (documento, imagem, \u00e1udio, v\u00eddeos etc.)</li> <li>Veracidade: a qualidade dos dados capturados pode variar muito, afetando a precis\u00e3o das an\u00e1lises.</li> <li>Valor: \u00e9 necess\u00e1rio transformar os dados em valor.</li> </ul> <p>Refer\u00eancias</p> <p>O que \u00e9 big data? O que s\u00e3o dados estruturados?</p>"},{"location":"00-fundamentos-de-dados/#2-desafios-do-big-data","title":"2. Desafios do Big Data","text":"<p>O Big Data apresenta desafios significativos, como:</p> <ul> <li>Armazenamento e gerenciamento de grandes volumes de dados.</li> <li>Integra\u00e7\u00e3o de diferentes fontes de dados.</li> <li>An\u00e1lise de grandes conjuntos de dados para obter insights.</li> <li>Necessidade de fornecer suporte para \u2018Fast Data\u2019 (capacidade de processar fluxos de dados em alta velocidade e tomar decis\u00f5es em tempo real).</li> </ul>"},{"location":"00-fundamentos-de-dados/#3-dados-transacionais","title":"3. Dados transacionais","text":"<p>Dados transacionais s\u00e3o informa\u00e7\u00f5es capturadas de transa\u00e7\u00f5es. Eles registram a hora da transa\u00e7\u00e3o, o local onde ocorreu, os pre\u00e7os dos itens comprados, a forma de pagamento empregada, os descontos, se houver, e outras quantidades e qualidades associadas \u00e0 transa\u00e7\u00e3o. Os dados transacionais geralmente s\u00e3o capturados no ponto de venda.</p> <p>O trabalho que \u00e9 realizado por sistemas transacionais \u00e9 normalmente conhecido com OLTP (Processamento de Transa\u00e7\u00f5es Online). O sistema OLTP tem como objetivo principal processar as transa\u00e7\u00f5es do banco de dados.</p> <p>Exemplos:</p> <ul> <li>OLTP: processar pedidos, atualizar estoque e gerenciar contas de clientes.</li> </ul> <p>Refer\u00eancias</p> <p>O que s\u00e3o dados transacionais? Qual \u00e9 a diferen\u00e7a entre OLAP e OLTP?</p>"},{"location":"00-fundamentos-de-dados/#4-dados-informacionais","title":"4. Dados informacionais","text":"<p>Dados informacionais possuem o objetivo de atender a necessidade da tomada de decis\u00e3o e para isso se baseia em grandes volumes de dados da base transacional.</p> <p>Nesse caso, o hist\u00f3rico \u00e9 essencial e necessita de muita armazenagem. E essas caracter\u00edsticas s\u00e3o denonimadas ao sistema OLAP (Processamento Anal\u00edtico Online). O objetivo principal do processamento anal\u00edtico on-line (OLAP) \u00e9 analisar dados agregados.</p> <p>Exemplos:</p> <ul> <li>OLAP: gerar relat\u00f3rios, realizar an\u00e1lises complexas de dados e identificar tend\u00eancias.</li> </ul> <p>Refer\u00eancias</p> <p>Modelagem de Dados Qual \u00e9 a diferen\u00e7a entre OLAP e OLTP?</p>"},{"location":"00-fundamentos-de-dados/#5-ingestao-de-dados","title":"5. Ingest\u00e3o de dados","text":"<p>A ingest\u00e3o de dados \u00e9 o processo de extrair dados de uma variedade de fontes e transferi-los para um local de destino onde podem ser depositados e analisados.</p> <p>A extra\u00e7\u00e3o pode ser feita de planilhas, webscrapping, aplicativos internos etc. Os destinos podem ser banco de dados, data warehouses etc.</p> <p>A ingest\u00e3o pode ser feita de duas maneiras, sendo elas:</p> <ul> <li>Ingest\u00e3o em tempo real (streaming): \u00fatil quando os dados s\u00e3o sens\u00edveis ao tempo. Os dados s\u00e3o extra\u00eddos, processados \u200b\u200be armazenados assim que s\u00e3o gerados para a tomada de decis\u00f5es em tempo real. Por exemplo, os dados adquiridos do app do Ita\u00fa deve ser monitorado continuamente para garantir a disponibilidade para o usu\u00e1rio.</li> <li>Ingest\u00e3o em lote (batch): os dados s\u00e3o movidos em intervalos agendado de forma recorrente. \u00c9 utilizado para processos repet\u00edveis. Por exemplo, relat\u00f3rios que precisam ser gerados todos os dias.</li> </ul>"},{"location":"00-fundamentos-de-dados/#6-processamento-de-dados","title":"6. Processamento de Dados","text":"<p>Processamento de dados \u00e9 um conjunto de atividades que visam organizar informa\u00e7\u00f5es. Neste t\u00f3pico, ser\u00e1 abordado ETL e ELT.</p>"},{"location":"00-fundamentos-de-dados/#61-processo-etl","title":"6.1 Processo ETL","text":"<ol> <li>Extra\u00e7\u00e3o dos dados brutos de v\u00e1rias fontes;</li> <li>Transforma\u00e7\u00e3o dos dados;</li> <li>Carregamento desses dados em um banco de dados de destino.</li> </ol>"},{"location":"00-fundamentos-de-dados/#62-processo-elt","title":"6.2 Processo ELT","text":"<ol> <li>Extra\u00e7\u00e3o dos dados brutos de v\u00e1rias fontes;</li> <li>Carregamento dos dados em eu estado natural em um data warehouse ou data lake;</li> <li>Transforma\u00e7\u00e3o dos dados conforme necess\u00e1rio enquanto est\u00e1 no sistema de destino.</li> </ol> <p>Com o ELT, toda a limpeza, transforma\u00e7\u00e3o e enriquecimento de dados ocorrem dentro do data warehouse. Voc\u00ea pode interagir e transformar os dados brutos quantas vezes forem necess\u00e1rias.</p>"},{"location":"00-fundamentos-de-dados/#63-historia-do-etl-e-elt","title":"6.3 Hist\u00f3ria do ETL e ELT","text":"<p>O ETL existe desde a d\u00e9cada de 1970, tornando-se especialmente popular com o surgimento dos data warehouses. No entanto, os data warehouses tradicionais exigiam processos ETL personalizados para cada fonte de dados.</p> <p>A evolu\u00e7\u00e3o das tecnologias de nuvem mudou o que era poss\u00edvel. Agora, as empresas podiam armazenar dados brutos ilimitados em grande escala e analis\u00e1-los posteriormente, conforme necess\u00e1rio. O ELT se tornou o m\u00e9todo moderno de integra\u00e7\u00e3o de dados para an\u00e1lises eficientes.</p>"},{"location":"00-fundamentos-de-dados/#64-principais-diferencas","title":"6.4 Principais diferen\u00e7as","text":"Descri\u00e7\u00e3o  ETL ELT Local de transforma\u00e7\u00e3o e carga Transforma dados em um servidor de processamento secund\u00e1rio. Carrega dados brutos diretamente no data warehouse de destino. Uma vez l\u00e1, voc\u00ea pode transformar os dados sempre que precisar. Compatibilidade de dados Adequado para dados estruturados que voc\u00ea pode representar em tabelas com linhas e colunas. Lida com todos os tipos de dados, incluindo dados n\u00e3o estruturados. Velocidade Possui uma etapa adicional antes de carregar dados no destino, que \u00e9 dif\u00edcil de escalar e desacelera o sistema \u00e0 medida que o tamanho dos dados.  O ELT \u00e9 mais r\u00e1pido que o ETL. Utiliza o poder de processamento e a paraleliza\u00e7\u00e3o que os data warehouses em nuvem oferecem para fornecer transforma\u00e7\u00e3o de dados em tempo real ou quase em tempo real para an\u00e1lise. Custos Exige o envolvimento da an\u00e1lise desde o in\u00edcio. \u00c9 necess\u00e1rio que os analistas planejem com anteced\u00eancia os relat\u00f3rios que desejam gerar e definam estruturas e formata\u00e7\u00e3o de dados. O tempo necess\u00e1rio para a configura\u00e7\u00e3o aumenta, o que aumenta os custos. Tem menos sistemas do que o ETL, pois todas as transforma\u00e7\u00f5es ocorrem no data warehouse de destino. Com menos sistemas, h\u00e1 menos para manter, resultando em uma pilha de dados mais simples e menores custos de configura\u00e7\u00e3o."},{"location":"00-fundamentos-de-dados/#65-quando-usar","title":"6.5 Quando usar","text":"<p>Ap\u00f3s vermos todas as vantagens, o ETL s\u00f3 teria necessidade em bancos de dados antigos. Fora isso, o ELT \u00e9 mais recomendado.</p> <p>Refer\u00eancias</p> <p>Qual \u00e9 a diferen\u00e7a entre ETL e ELT?</p>"},{"location":"00-fundamentos-de-dados/#7-camadas-de-dados","title":"7. Camadas de dados","text":"<p>As camadas SOR (System of Record), SOT (System of Transformation) e SPEC (Specialized Processing Engines) gerenciam diferentes est\u00e1gios do ciclo de vida dos dados, desde o armazenamento bruto at\u00e9 a transforma\u00e7\u00e3o e o processamento especializado. Abaixo breve descri\u00e7\u00e3o das camadas:</p> <ul> <li>Camada SOR (System of Record): Respons\u00e1vel pelo armazenamento inicial dos dados brutos, funcionando como a principal refer\u00eancia para informa\u00e7\u00f5es ainda n\u00e3o processadas;</li> <li>Camada SOT (System of Transformation): Destinada \u00e0 transforma\u00e7\u00e3o, limpeza e prepara\u00e7\u00e3o dos dados para an\u00e1lise. Nessa etapa, s\u00e3o aplicadas regras de neg\u00f3cios e ajustes necess\u00e1rios para garantir a qualidade das informa\u00e7\u00f5es;</li> <li>Camada SPEC (Specialized Processing Engines): Focada em processamentos avan\u00e7ados ou especializados, como execu\u00e7\u00e3o de algoritmos de machine learning, gera\u00e7\u00e3o de novas features a partir dos dados e transforma\u00e7\u00f5es espec\u00edficas para aprofundar as an\u00e1lises e gerar insights.</li> </ul> <p>Desenho de camadas de dados, processamento e ingest\u00e3o</p> <p>Refer\u00eancias</p> <p>Como o Ita\u00fa reduziu custos de armazenamento no Amazon S3</p>"},{"location":"00-fundamentos-de-dados/#8-tipos-de-dados","title":"8. Tipos de dados","text":"<p>Existem 3 tipos de dados, sendo:</p> <ul> <li>Dados Estruturados: S\u00e3o dados organizados em tabelas, como em bancos de dados. Cada dado tem um formato espec\u00edfico, como n\u00fameros ou textos, o que facilita armazenar e acessar.</li> <li>Dados Semi-Estruturados: S\u00e3o dados que t\u00eam alguma organiza\u00e7\u00e3o, mas n\u00e3o t\u00e3o r\u00edgida como em tabelas. Exemplos s\u00e3o arquivos JSON e XML, que t\u00eam uma estrutura, mas n\u00e3o s\u00e3o fixos como os dados estruturados.</li> <li>Dados N\u00e3o Estruturados: S\u00e3o dados sem um formato organizado, como fotos, v\u00eddeos, e-mails e postagens em redes sociais. Eles s\u00e3o mais dif\u00edceis de analisar sem ferramentas espec\u00edficas.</li> </ul>"},{"location":"00-fundamentos-de-dados/#9-formatos-de-dados","title":"9. Formatos de dados","text":"<p>Os formatos mais utilizados no cen\u00e1rio de democratiza\u00e7\u00e3o de dados s\u00e3o:</p> Formato Tipo de Armazenamento Estrutura Principais Vantagens Exemplos de Uso LZO Compress\u00e3o de dados N\u00e3o estruturado Compress\u00e3o r\u00e1pida e eficiente para grandes volumes de dados. Compress\u00e3o de logs e grandes datasets em Hadoop. Parquet Armazenamento colunar Estruturado (colunas) Leitura r\u00e1pida de grandes volumes de dados, \u00f3timo para consultas de colunas espec\u00edficas. Armazenamento de dados em data lakes, Big Data. ORC Armazenamento colunar Estruturado (colunas) Alta compress\u00e3o e performance para leitura e processamento. Armazenamento de grandes logs, an\u00e1lise de dados no Hive. Avro Serializa\u00e7\u00e3o de dados Estruturado (linha) Compacto, eficiente para transmiss\u00e3o de dados em sistemas distribu\u00eddos. Arquivos de eventos e streaming de dados. XML Texto Hier\u00e1rquico (tags) Facilita a leitura e escrita, mas mais pesado para grandes volumes de dados. Arquivos de configura\u00e7\u00e3o, trocas de dados entre sistemas. JSON Texto Estruturado (pares chave-valor) Leve, f\u00e1cil de ler e escrever, amplamente usado em APIs e integra\u00e7\u00e3o. APIs, armazenamento de configura\u00e7\u00f5es, respostas de servi\u00e7os web. <p>Info</p> <ul> <li> <p>Compress\u00e3o de Dados: Refere-se a reduzir o tamanho dos arquivos para otimizar o armazenamento ou a transfer\u00eancia. Em vez de armazenar um arquivo grande, voc\u00ea usa um algoritmo para comprimi-lo, tornando-o mais leve. Exemplo: Compactar um arquivo de log de dados (como .gz) para reduzir o espa\u00e7o usado no armazenamento ou acelerar a transfer\u00eancia entre sistemas.</p> </li> <li> <p>Serializa\u00e7\u00e3o de Dados: Transformar dados estruturados em um formato que pode ser facilmente armazenado ou transmitido, como transformar um objeto ou estrutura em bytes. Exemplo: Serializar um dataframe em formato Parquet ou Avro para armazen\u00e1-lo em um banco de dados ou transmitir entre sistemas distribu\u00eddos.</p> </li> </ul>"},{"location":"00-fundamentos-de-dados/#10-hadoop","title":"10. Hadoop","text":"<p>De acordo com o site oficial da Apache, o Hadoop \u00e9 um framework que permite processamento distribu\u00eddo de big data ao longo de clusters. Sua arquitetura comporta uma alta escalabilidade, permeando desde servidores individuais at\u00e9 milhares de m\u00e1quinas, cada uma oferecendo armazenamento e poder computacional local.</p> <p>O Hadoop possui tr\u00eas elementos fundamentais para armazenar e processar de forma distribu\u00edda grandes quantidade de dados, sendo:</p> <ul> <li>HDFS (Hadoop Distributed File System): sistema de armazenamento distribu\u00eddo do Hadoop.</li> <li>MapReduce: modelo de programa\u00e7\u00e3o baseado em mappers e reducers.</li> <li>YARN (Yet Another Resource Negotiator): gerenciador de recursos do cluster Hadoop.</li> </ul> <p>Veremos detalhadamente cada um dos tr\u00eas elementos.</p> <p>Refer\u00eancias</p> <p>Hadoop Core: HDFS</p>"},{"location":"00-fundamentos-de-dados/#101-hdfs","title":"10.1 HDFS","text":"<p>O HDFS \u00e9 um sistema de arquivos constru\u00eddo para armazenar grandes quantidades de dados com acesso atrav\u00e9s de padr\u00f5es de streaming executado em clusters de hardware commodity. Elementos desta defini\u00e7\u00e3o:</p> <ul> <li>Padr\u00f5es de streaming: O HDFS segue o modelo write once, read many, onde grandes quantidades de dados s\u00e3o escritas uma vez e lidas v\u00e1rias vezes.</li> <li>Hardware commodity: O Hadoop funciona em clusters de m\u00e1quinas simples, aproveitando replica\u00e7\u00e3o e alta disponibilidade para reduzir custos.</li> </ul> <p>O HDFS contribui para solu\u00e7\u00f5es de armazenamento de grandes quantidades de dados da seguinte forma:</p> <p>Imagine um arquivo de texto de 50 terabytes de volume que precisa ser armazenado e posteriormente transformado em busca de novos insights dentro de uma companhia.</p> <p>De forma direta, este massivo conjunto de dados n\u00e3o pode ser comportado por HDs ou SSDs com a tecnologia existente at\u00e9 o momento de escrita deste reposit\u00f3rio. Assim, na impossibilidade de armazenar grandes volumes em uma \u00fanica m\u00e1quina, a resposta mais objetiva poss\u00edvel \u00e9 a utiliza\u00e7\u00e3o de m\u00faltiplas m\u00e1quinas capazes de comportar, cada uma, uma parcela (ou uma parti\u00e7\u00e3o) do arquivo de dados. \u00c1 esta organiza\u00e7\u00e3o de m\u00faltiplas m\u00e1quinas d\u00e1-se o nome de cluster.</p>"},{"location":"00-fundamentos-de-dados/#1011-hdfs-replicacao-de-dados-no-cluster","title":"10.1.1 HDFS - Replica\u00e7\u00e3o de Dados no Cluster","text":"<p>Uma caracter\u00edstica interessante do HDFS s\u00e3o seus blocos de armazenamento de 128MB (ou 2510MB em alguns cen\u00e1rios) do hardware commodity e replica\u00e7\u00e3o de dados no cluster. Se um computador do cluster hadoop falhar, o HDFS  automaticamente ir\u00e1 garantir que os dados solicitados pela requisi\u00e7\u00e3o ser\u00e3o retornados atrav\u00e9s de uma fonte alternativa de replica\u00e7\u00e3o.</p>"},{"location":"00-fundamentos-de-dados/#1012-hdfs-arquitetura","title":"10.1.2 HDFS - Arquitetura","text":"<p>Quando um client realiza uma requisi\u00e7\u00e3o ao cluster Hadoop, estar\u00e1 eventualmente interagindo com diferentes tipos de n\u00f3s do sistema que atuam em um padr\u00e3o master-worker, sendo eles:</p> <ul> <li>Namenodes (master): gerencia toda a \u00e1rvore do sistema de armazenamento e os metadados para todos os arquivos e diret\u00f3rios presentes. Os namenodes mant\u00e9m um arquivo conhecido como edit log respons\u00e1vel por registrar informa\u00e7\u00f5es sobre o que \u00e9 criado, mantido ou alterado no cluster. Al\u00e9m disso, possuem informa\u00e7\u00f5es de todos os datanodes do sistema.</li> <li>Datanodes (worker): atuam como os verdadeiros trabalhadores do cluster. S\u00e3o eles quem armazenam os dados em blocos e retornam, sempre que solicitados, informa\u00e7\u00f5es para o namenode para que este saiba exatamente a localiza\u00e7\u00e3o dos arquivos.</li> </ul> <p>Refer\u00eancias</p> <p>Hadoop Core: HDFS</p>"},{"location":"00-fundamentos-de-dados/#102-mapreduce","title":"10.2 MapReduce","text":"<p>MapReduce \u00e9 um modelo de programa\u00e7\u00e3o desenhado para processar grandes volumes de dados em paralelo, dividindo o trabalho em m\u00faltiplos processadores em um cluster de computadores.</p> <p>Quando a escala dos dados beira os Petabytes, cen\u00e1rio comum em Big Data, o tempo de processamento com um HDD ou SSD \u00e9 insuficiente para comportar as necessidades do mundo moderno e \u00e9 por isso que existe o MapReduce.</p> <p>Basicamente, o MapReduce possui 5 etapas, sendo elas:</p> <ul> <li>Input Data: dados que ser\u00e3o utilizados para o MapReduce.</li> <li>Split Data: dados s\u00e3o divididos.</li> <li>Map: etapa onde cada task recebe uma por\u00e7\u00e3o de dados, ou seja, o conjunto de dados est\u00e1 armazenado de maneira distribu\u00edda em diferentes blocos e em diferentes n\u00f3s em um cluster de computadores.</li> <li>Shuffle and Sort: etapa onde os dados s\u00e3o unificados em cada n\u00f3 do cluster de modo a criar um input facilitado para a subsequente tarefa de reduce.</li> <li>Reduce: etapa onde s\u00e3o executadas as agrega\u00e7\u00f5es e sumariza\u00e7\u00f5es dos dados, tendo seus resultados unificados para proporcionar uma s\u00e1ida final.</li> </ul> <pre><code>graph LR\n  A[Input Data] --&gt;|Map| B[Mapper 1]\n  A --&gt;|Map| C[Mapper 2]\n  A --&gt;|Map| D[Mapper 3]\n\n  B --&gt;|Shuffle &amp; Sort| E[Reducer 1]\n  C --&gt;|Shuffle &amp; Sort| E\n  D --&gt;|Shuffle &amp; Sort| E\n\n  E --&gt;|Final Output| F[Result Data]</code></pre> <p>Refer\u00eancias</p> <p>MapReduce</p>"},{"location":"00-fundamentos-de-dados/#103-yarn","title":"10.3 YARN","text":"<p>O YARN \u00e9 o sistema que gerencia os recursos no cluster de computadores, decidindo quem executa as tarefas, quando as mesmas ser\u00e3o executadas, quais n\u00f3s est\u00e3o dispon\u00edveis para receber demandas e quais est\u00e3o totalmente ocupados. O YARN \u00e9 como o cora\u00e7\u00e3o que permite o funcionamento do cluster.</p> <p>Refer\u00eancias</p> <p>Yarn</p>"},{"location":"00-fundamentos-de-dados/#11-apache-spark","title":"11. Apache Spark","text":"<p>O Spark \u00e9 um sistema de processamento distribu\u00eddo de big data que foi criado para resolver as limita\u00e7\u00f5es do MapReduce. Antes do Spark, cada etapa exigia uma leitura e grava\u00e7\u00e3o no disco, tornado mais lento, devido \u00e0 lat\u00eancia da E/S do disco. J\u00e1 no Spark, o processamento \u00e9 feito na mem\u00f3ria, reduzindo o n\u00famero de etapas em uma tarefa e reutilizando dados em v\u00e1rias opera\u00e7\u00f5es paralelas.</p>"},{"location":"00-fundamentos-de-dados/#111-workloads-do-apache-spark","title":"11.1 Workloads do Apache Spark","text":"<ul> <li>Spark Core: base para a plataforma.</li> <li>Spark SQL: consultas interativas.</li> <li>Spark Streaming: an\u00e1lises em tempo real.</li> <li>Spark MLlib: para machine learning.</li> <li>Spark GraphX: processamento de gr\u00e1ficos.</li> </ul> <p>Refer\u00eancias</p> <p>O que \u00e9 Apache Spark?</p>"},{"location":"00-fundamentos-de-dados/#112-beneficios-do-apache-spark","title":"11.2 Benef\u00edcios do Apache Spark","text":"<ul> <li>R\u00e1pido: por meio do armazenamento em cache na mem\u00f3ria.</li> <li>Para desenvolvedores: sustenta de modo nativo Java, Scala, R e Python, oferecendo a voc\u00ea v\u00e1rias linguagens para a cria\u00e7\u00e3o de aplicativos.</li> </ul>"},{"location":"00-fundamentos-de-dados/#113-apache-hadoop-vs-apache-spark","title":"11.3 Apache Hadoop vs Apache Spark","text":"<p>O Hadoop possui o HDFS para armazenamento e YARN para gerenciar recusos de computa\u00e7\u00e3o.</p> <p>O Spark n\u00e3o possui sistema pr\u00f3prio de armazenamento, mas pode consultar no HDFS, Redshift, S3 etc.</p>"},{"location":"00-fundamentos-de-dados/#12-qualidade-dos-dados","title":"12. Qualidade dos dados","text":"<p>Qualidade de Dados (Data Quality, em ingl\u00eas) \u00e9 uma avalia\u00e7\u00e3o da precis\u00e3o, completude, consist\u00eancia, confiabilidade e atualidade dos dados. Em outras palavras, refere-se ao grau de excel\u00eancia dos dados</p>"},{"location":"00-fundamentos-de-dados/#121-metricas-de-qualidade","title":"12.1 M\u00e9tricas de qualidade","text":"<p>1. Completude: Isso se refere a se todos os dados necess\u00e1rios est\u00e3o dispon\u00edveis. Se houver campos obrigat\u00f3rios ausentes em um conjunto de dados, ele n\u00e3o ser\u00e1 considerado completo.</p> <p>2. Consist\u00eancia: Os dados precisam ser consistentes, o que significa que n\u00e3o devem existir discrep\u00e2ncias quando comparados entre diferentes conjuntos de dados ou diferentes partes do mesmo conjunto de dados.</p> <p>3. Conformidade: Isso se refere ao grau em que os dados aderem a padr\u00f5es especificados, conven\u00e7\u00f5es e regras de neg\u00f3cios. Por exemplo, um n\u00famero de telefone deve estar em um formato v\u00e1lido.</p> <p>4. Integridade: Isso se refere \u00e0 validade e consist\u00eancia de relacionamentos entre entidades e registros de dados, como refer\u00eancias cruzadas entre tabelas em um banco de dados relacional.</p> <p>5. Precis\u00e3o: Isso se refere ao grau em que os dados representam a realidade ou a verdade. A precis\u00e3o pode ser dif\u00edcil de medir, pois requer um ponto de refer\u00eancia verdadeiro.</p> <p>6. Atualidade: Isso se refere \u00e0 relev\u00e2ncia dos dados no tempo. Isso pode variar dependendo do contexto; por exemplo, dados de vendas de um dia atr\u00e1s podem ser considerados atuais para algumas empresas, enquanto para outras empresas esses dados podem ser considerados desatualizados.</p> <p>Refer\u00eancias</p> <p>As 6 Dimens\u00f5es da Qualidade de Dados (Data Quality)</p>"},{"location":"01-perguntas-de-eng-dados-aws/","title":"Perguntas de Engenharia de Dados na AWS","text":""},{"location":"01-perguntas-de-eng-dados-aws/#1-conceitos-fundamentais-de-engenharia-de-dados","title":"1. Conceitos Fundamentais de Engenharia de Dados","text":""},{"location":"01-perguntas-de-eng-dados-aws/#11-como-voce-lidaria-com-dados-em-tempo-real-vs-dados-em-batch","title":"1.1 Como voc\u00ea lidaria com dados em tempo real vs. dados em batch?","text":"<p>Dados em Tempo Real</p> <p>Esses dados s\u00e3o processados \u00e0 medida que chegam, com baixa lat\u00eancia. Para isso, na AWS, podemos usar:</p> <ul> <li>Amazon Kinesis Data Streams: Para coletar e processar grandes fluxos de dados em tempo real, como logs de eventos de aplicativos ou transa\u00e7\u00f5es financeiras.</li> <li>AWS Lambda: Para processamento serverless de eventos em tempo real, ideal para transformar dados conforme eles chegam.</li> <li>Amazon DynamoDB Streams: Para capturar mudan\u00e7as em uma tabela do DynamoDB em tempo real e process\u00e1-las automaticamente.</li> </ul> <p>Exemplo: gitsync e apisybc.</p> <p>Dados em Batch</p> <p>Esses dados s\u00e3o processados em intervalos de tempo espec\u00edficos. Na AWS, os principais servi\u00e7os s\u00e3o:</p> <ul> <li>Amazon S3 + AWS Glue: Para armazenar grandes volumes de dados e preparar (ETL) esses dados para an\u00e1lise.</li> <li>Amazon EMR (Elastic MapReduce): Para processar grandes volumes de dados usando frameworks como Apache Spark ou Hadoop.</li> <li>Amazon Athena: Para consultar diretamente dados armazenados no S3, sem necessidade de configura\u00e7\u00e3o de servidor.</li> <li>Amazon RDS/Aurora: Para processar e analisar dados armazenados em bancos de dados relacionais em hor\u00e1rios espec\u00edficos.</li> <li>AWS Batch: Para gerenciar e escalar automaticamente jobs em batch.</li> </ul> <p>Exemplo pr\u00e1tico: Em uma empresa de an\u00e1lise de marketing, voc\u00ea pode armazenar todos os dados de campanhas publicit\u00e1rias em Amazon S3, usar o AWS Glue para transformar esses dados e o Amazon Athena para gerar relat\u00f3rios di\u00e1rios de performance.</p>"},{"location":"01-perguntas-de-eng-dados-aws/#12-qual-a-diferenca-do-amazon-rds-para-amazon-redshift","title":"1.2 Qual a diferen\u00e7a do Amazon RDS para Amazon Redshift?","text":"<p>Amazon RDS</p> <ul> <li>Tipo: Banco de dados relacional gerenciado.</li> <li>Uso: Otimizado para transa\u00e7\u00f5es OLTP (Online Transaction Processing).</li> <li>Exemplos de Bancos Suportados: MySQL, PostgreSQL, MariaDB, SQL Server, Oracle e Amazon Aurora.</li> <li>Casos de Uso: Aplica\u00e7\u00f5es web, sistemas de gest\u00e3o empresarial, aplicativos m\u00f3veis, e-commerce.</li> </ul> <p>Amazon Redshift</p> <ul> <li>Tipo: Data Warehouse gerenciado.</li> <li>Uso: Otimizado para consultas anal\u00edticas (OLAP - Online Analytical Processing).</li> <li>Armazenamento: Baseado em colunas, ideal para grandes volumes de dados.</li> <li>Casos de Uso: An\u00e1lise de dados em larga escala, relat\u00f3rios de BI (Business Intelligence).</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#2-cloud-computing-na-aws","title":"2. Cloud Computing na AWS","text":""},{"location":"01-perguntas-de-eng-dados-aws/#21-quais-sao-os-principais-servicos-da-aws-para-engenharia-de-dados","title":"2.1 Quais s\u00e3o os principais servi\u00e7os da AWS para engenharia de dados?","text":"<ul> <li>Amazon S3: Armazenamento escal\u00e1vel de objetos.</li> <li>AWS Glue: Servi\u00e7o de ETL gerenciado.</li> <li>Amazon Redshift: Data warehouse em nuvem.</li> <li>Amazon Athena: Consulta de dados em S3 usando SQL.</li> <li>Amazon EMR: Cluster gerenciado para Big Data.</li> <li>Amazon Kinesis: Ingest\u00e3o e processamento de dados em tempo real.</li> <li>AWS Lambda: Execu\u00e7\u00e3o de c\u00f3digo sem servidor.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#22-quais-sao-as-diferencas-entre-o-aws-glue-e-o-amazon-emr-na-aws-para-processamento-de-big-data","title":"2.2 Quais s\u00e3o as diferen\u00e7as entre o AWS Glue e o Amazon EMR na AWS para processamento de big data?","text":"<p>Tanto o AWS Glue quanto o Amazon EMR (Elastic MapReduce) s\u00e3o servi\u00e7os da AWS para processamento de big data, mas eles possuem caracter\u00edsticas e casos de uso espec\u00edficos.</p> <p>AWS Glue</p> <ul> <li>Tipo: Servi\u00e7o de ETL (Extract, Transform, Load) totalmente gerenciado.</li> <li>Interface: Baseada em servidorless, sem necessidade de gerenciamento de infraestrutura.</li> <li>Casos de Uso: Transforma\u00e7\u00e3o de dados, prepara\u00e7\u00e3o de dados para an\u00e1lise, integra\u00e7\u00e3o com Amazon S3 e Amazon Athena.</li> <li>Linguagem Suportada: PySpark (Python + Spark) e Scala.</li> <li> <p>Principais Funcionalidades:</p> </li> <li> <p>Glue Data Catalog para metadados e esquema dos dados.</p> </li> <li>Glue Studio com interface gr\u00e1fica para criar pipelines ETL.</li> <li>Glue Crawlers para descoberta autom\u00e1tica de esquemas de dados.</li> <li>Escalabilidade: Autom\u00e1tica (voc\u00ea n\u00e3o gerencia n\u00f3s).</li> <li>Modelo de Custo: Paga-se por segundo de execu\u00e7\u00e3o.</li> </ul> <p>Exemplo Pr\u00e1tico: Voc\u00ea precisa transformar arquivos JSON armazenados no Amazon S3 em tabelas Parquet otimizadas para consulta com Amazon Athena.</p> <p>Amazon EMR (Elastic MapReduce)</p> <ul> <li>Tipo: Cluster gerenciado para frameworks de big data (Hadoop, Spark, Presto, HBase, Flink, e mais).</li> <li>Interface: Controle completo do cluster (EC2 gerenciado) e configura\u00e7\u00e3o personalizada.</li> <li>Casos de Uso: Processamento intensivo de big data, machine learning distribu\u00eddo, an\u00e1lise de logs, e pipelines de dados complexos.</li> <li>Linguagem Suportada: Python (Spark), Scala, Java, SQL, R, e mais, dependendo do framework.</li> <li> <p>Principais Funcionalidades:</p> </li> <li> <p>Suporte para m\u00faltiplos frameworks (Hadoop, Spark, Presto, etc.).</p> </li> <li>Controle total sobre o hardware (tipo de inst\u00e2ncia, n\u00famero de n\u00f3s, configura\u00e7\u00e3o de mem\u00f3ria/CPU).</li> <li>Suporte para workloads altamente personalizadas.</li> <li>Escalabilidade: Manual ou autom\u00e1tica (Auto Scaling).</li> <li>Modelo de Custo: Paga-se por inst\u00e2ncia EC2 e armazenamento associado.</li> </ul> <p>Exemplo Pr\u00e1tico: Voc\u00ea precisa treinar um modelo de machine learning em Spark MLlib com um cluster de 100 n\u00f3s para an\u00e1lise de logs de servidores.</p> <p>Quando usar cada um?</p> Crit\u00e9rio AWS Glue Amazon EMR Simplicidade Simples, serverless. Complexo, com controle total do cluster. Custo Custo controlado (serverless). Pode ser mais caro (depende do tamanho do cluster). Flexibilidade Limitado a ETL (Spark, Scala, Python). Altamente flex\u00edvel (v\u00e1rios frameworks). Casos de Uso ETL, prepara\u00e7\u00e3o de dados, integra\u00e7\u00e3o com S3/Athena. Processamento de big data, machine learning, pipelines complexos. Gerenciamento Autom\u00e1tico (serverless). Manual (voc\u00ea gerencia os n\u00f3s). Escalabilidade Autom\u00e1tica. Manual ou autom\u00e1tica (Auto Scaling). <p>Como escolher?</p> <ul> <li>Use AWS Glue se voc\u00ea precisa de uma solu\u00e7\u00e3o de ETL f\u00e1cil de configurar e gerenciada para transformar e preparar dados.</li> <li>Use Amazon EMR se voc\u00ea precisa de controle total sobre o cluster e suporte para frameworks adicionais (como Hadoop, Spark MLlib, Presto).</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#23-como-voce-garantiria-a-seguranca-dos-dados-armazenados-no-s3","title":"2.3 Como voc\u00ea garantiria a seguran\u00e7a dos dados armazenados no S3?","text":"<p>Para garantir a seguran\u00e7a dos dados armazenados no Amazon S3, podemos adotar as seguintes pr\u00e1ticas:</p> <p>Controle de Acesso</p> <ul> <li>Pol\u00edticas do IAM: Use pol\u00edticas de controle de acesso refinadas para usu\u00e1rios e grupos.</li> <li>Bucket Policies: Defina pol\u00edticas espec\u00edficas de acesso ao bucket, limitando quem pode visualizar ou modificar os dados.</li> <li>Access Control Lists (ACLs): Configure permiss\u00f5es em n\u00edvel de objeto, se necess\u00e1rio.</li> </ul> <p>Criptografia</p> <ul> <li>Criptografia em repouso: Habilite a criptografia autom\u00e1tica com AWS Key Management Service (KMS) ou chaves gerenciadas pelo S3 (SSE-S3).</li> <li>Criptografia em tr\u00e2nsito: Use HTTPS (TLS) para transferir dados de forma segura.</li> </ul> <p>Monitoramento e Auditoria</p> <ul> <li>Amazon CloudTrail: Ative o CloudTrail para registrar todas as a\u00e7\u00f5es realizadas no S3, garantindo rastreabilidade.</li> <li>Amazon Macie: Identifique dados confidenciais e receba alertas sobre poss\u00edveis exposi\u00e7\u00f5es.</li> <li>S3 Access Logs: Habilite o registro de acessos para analisar quem acessou o bucket.</li> </ul> <p>Prote\u00e7\u00e3o Contra Exclus\u00e3o</p> <ul> <li>Versionamento: Ative o versionamento para manter c\u00f3pias antigas dos objetos.</li> <li>Bloqueio de Objetos: Use o S3 Object Lock para proteger objetos contra exclus\u00e3o acidental.</li> <li>MFA Delete: Exija autentica\u00e7\u00e3o multifator para exclus\u00e3o de objetos cr\u00edticos.</li> </ul> <p>Melhores Pr\u00e1ticas Adicionais</p> <ul> <li>Princ\u00edpio de Menor Privil\u00e9gio: Conceda apenas as permiss\u00f5es necess\u00e1rias para cada usu\u00e1rio.</li> <li>Segrega\u00e7\u00e3o de Buckets: Mantenha dados sens\u00edveis e p\u00fablicos em buckets separados.</li> <li>Avalia\u00e7\u00e3o Regular: Realize auditorias peri\u00f3dicas para garantir que as permiss\u00f5es est\u00e3o configuradas corretamente.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#24-como-voce-configuraria-um-pipeline-de-ingestao-de-dados-usando-glue-e-s3","title":"2.4 Como voc\u00ea configuraria um pipeline de ingest\u00e3o de dados usando Glue e S3?","text":"<ul> <li>Configuraria uma tarefa do Glue para extrair dados de uma fonte (como RDS), transform\u00e1-los e carreg\u00e1-los no S3.</li> <li>Definiria gatilhos para automa\u00e7\u00e3o (hor\u00e1rio, eventos S3).</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#3-design-e-arquitetura-de-dados","title":"3. Design e Arquitetura de Dados","text":""},{"location":"01-perguntas-de-eng-dados-aws/#31-como-voce-projetaria-uma-solucao-de-ingestao-de-dados-escalavel-na-aws","title":"3.1 Como voc\u00ea projetaria uma solu\u00e7\u00e3o de ingest\u00e3o de dados escal\u00e1vel na AWS?","text":"<ul> <li>Utilizaria Amazon S3 para armazenamento bruto.</li> <li>AWS Glue para transforma\u00e7\u00e3o de dados.</li> <li>Amazon Kinesis para ingest\u00e3o em tempo real.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#32-como-voce-projetaria-uma-arquitetura-para-processar-dados-em-tempo-real","title":"3.2 Como voc\u00ea projetaria uma arquitetura para processar dados em tempo real?","text":"<ul> <li>Utilizaria Amazon Kinesis para ingest\u00e3o.</li> <li>AWS Lambda para processamento em tempo real.</li> <li>Amazon DynamoDB ou S3 para armazenamento.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#33-quais-sao-as-boas-praticas-para-particionar-e-organizar-dados-em-um-data-lake-no-s3","title":"3.3 Quais s\u00e3o as boas pr\u00e1ticas para particionar e organizar dados em um data lake no S3?","text":"<ul> <li>Usar prefixos de parti\u00e7\u00e3o (por exemplo, ano/m\u00eas/dia).</li> <li>Definir pol\u00edticas de ciclo de vida para arquivamento.</li> <li>Manter uma estrutura de pasta padronizada.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#4-performance-e-otimizacao","title":"4. Performance e Otimiza\u00e7\u00e3o","text":""},{"location":"01-perguntas-de-eng-dados-aws/#41-como-otimizar-uma-consulta-no-redshift","title":"4.1 Como otimizar uma consulta no Redshift?","text":"<ul> <li>Uso de sort keys e distribution keys.</li> <li>Compress\u00e3o de colunas.</li> <li>Materialized views.</li> </ul> <p>O que s\u00e3o Sort Keys e Distribution Keys?</p> <ul> <li> <p>Sort Keys: Colunas usadas para ordenar os dados fisicamente na tabela, otimizando a velocidade de leitura para consultas frequentes que filtram ou ordenam por essas colunas.</p> </li> <li> <p>Exemplo: Se voc\u00ea usar uma coluna <code>anomesdia</code> como sort key e orden\u00e1-la de forma crescente, o Redshift organizar\u00e1 os dados fisicamente nessa ordem. Consultas filtrando por datas espec\u00edficas ser\u00e3o muito mais r\u00e1pidas, pois o Redshift ler\u00e1 diretamente o intervalo necess\u00e1rio.</p> </li> <li> <p>Distribution Keys: Definem como os dados s\u00e3o distribu\u00eddos entre os n\u00f3s do cluster Redshift, garantindo que consultas sejam mais eficientes, minimizando o movimento de dados entre n\u00f3s.</p> </li> </ul> <p>O que \u00e9 Compress\u00e3o de Colunas?</p> <p>\u00c9 a aplica\u00e7\u00e3o de algoritmos de compress\u00e3o (como LZO, ZSTD) nas colunas da tabela, reduzindo o espa\u00e7o de armazenamento e acelerando a leitura dos dados. Existem dois tipos principais.</p> <p>Compress\u00e3o sem perda (Lossless Compression):</p> <ul> <li>N\u00e3o perde nenhum dado durante a compress\u00e3</li> <li>Ideal para dados cr\u00edticos (textos, c\u00f3digos, tabelas).</li> <li>Exemplo: Algoritmo LZO (Lempel-Ziv-Oberhumer) e ZSTD (Zstandard), muito usados em bancos de dados como Redshift.</li> </ul> <p>Como Funciona:</p> <ul> <li>Identifica padr\u00f5es repetidos nos dados.</li> <li>Substitui padr\u00f5es por refer\u00eancias menores.</li> <li>Ao descomprimir, restaura os dados originais exatamente como eram.</li> </ul> <p>Exemplo</p> <p>Dados Originais: AAAAAABBBCCCCCC</p> <p>Ap\u00f3s Compress\u00e3o: A6B3C6</p> <p>Compress\u00e3o com perda (Lossy Compression):</p> <ul> <li>Perde alguns dados durante a compress\u00e3o.</li> <li>Usada em imagens, v\u00eddeos e \u00e1udio (onde uma pequena perda de qualidade \u00e9 aceit\u00e1vel).</li> <li>Exemplo: JPEG (imagens), MP3 (\u00e1udio).</li> </ul> <p>O que s\u00e3o Materialized Views?</p> <ul> <li>S\u00e3o visualiza\u00e7\u00f5es que armazenam os resultados de uma consulta em cache, permitindo acesso mais r\u00e1pido aos dados processados, pois evitam a necessidade de reexecutar a consulta toda vez.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#42-como-melhorar-o-desempenho-de-uma-pipeline-de-etl-no-glue","title":"4.2 Como melhorar o desempenho de uma pipeline de ETL no Glue?","text":"<ul> <li>Utilizando \"pushdown predicate\" para filtrar dados.</li> <li>Habilitando a op\u00e7\u00e3o de paralelismo din\u00e2mico.</li> </ul> <p>pushdown predicate</p> <p>Pushdown Predicate \u00e9 uma t\u00e9cnica de otimiza\u00e7\u00e3o em pipelines de ETL que pode ser aplicado no Glue. Em vez de carregar todos os dados para processamento e aplicar filtros depois, o Pushdown Predicate aplica os filtros diretamente na fonte de dados. Isso reduz a quantidade de dados transferidos e processados, acelerando o ETL.</p> <p>O filtro sempre \u00e9 feito, mas:</p> <ul> <li> <p>Sem pushdown, o Glue poderia ler todos os dados da fonte e s\u00f3 depois aplicar o filtro, o que \u00e9 custoso.</p> </li> <li> <p>Com pushdown, o filtro \u00e9 \u201cempurrado para baixo\u201d at\u00e9 a fonte, ou seja, o banco de dados (ou sistema de arquivos) j\u00e1 retorna apenas os dados filtrados.</p> </li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#43-o-que-e-particionamento-de-dados-e-como-ele-ajuda-no-desempenho","title":"4.3 O que \u00e9 particionamento de dados e como ele ajuda no desempenho?","text":"<ul> <li>Particionamento \u00e9 a divis\u00e3o dos dados em segmentos menores.</li> <li>Melhora o desempenho de consultas ao permitir leitura seletiva.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#5-monitoramento-e-seguranca","title":"5. Monitoramento e Seguran\u00e7a","text":""},{"location":"01-perguntas-de-eng-dados-aws/#51-como-voce-configuraria-politicas-de-acesso-para-proteger-seus-dados-no-s3","title":"5.1 Como voc\u00ea configuraria pol\u00edticas de acesso para proteger seus dados no S3?","text":"<ul> <li>Usando pol\u00edticas de bucket e pol\u00edticas de IAM.</li> <li>Ativando o bloqueio de acesso p\u00fablico.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#52-quais-sao-as-melhores-praticas-para-monitorar-uma-pipeline-de-dados-na-aws","title":"5.2 Quais s\u00e3o as melhores pr\u00e1ticas para monitorar uma pipeline de dados na AWS?","text":"<ul> <li>Usando Amazon CloudWatch para m\u00e9tricas e alarmes.</li> <li>Habilitando o logging detalhado no Glue.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#53-como-voce-implementaria-controle-de-versao-em-uma-pipeline-de-dados","title":"5.3 Como voc\u00ea implementaria controle de vers\u00e3o em uma pipeline de dados?","text":"<ul> <li>Usando versionamento em S3.</li> <li>Gerenciando vers\u00f5es de c\u00f3digo com Git.</li> </ul>"},{"location":"01-perguntas-de-eng-dados-aws/#6-perguntas-praticas-e-situacoes-de-problemas","title":"6. Perguntas Pr\u00e1ticas e Situa\u00e7\u00f5es de Problemas","text":""},{"location":"01-perguntas-de-eng-dados-aws/#61-como-voce-identificaria-e-corrigiria-falhas-em-uma-pipeline-de-dados-em-producao","title":"6.1 Como voc\u00ea identificaria e corrigiria falhas em uma pipeline de dados em produ\u00e7\u00e3o?","text":"<p>Identifica\u00e7\u00e3o de Falhas</p> <p>Monitoramento Cont\u00ednuo:</p> <ul> <li>Use Amazon CloudWatch para monitorar m\u00e9tricas e criar alarmes que notificam quando h\u00e1 falhas, lentid\u00e3o ou comportamentos anormais na pipeline.</li> <li>Habilite logs detalhados nos servi\u00e7os envolvidos (por exemplo, logs do AWS Glue, AWS Lambda, Kinesis, etc.).</li> <li>Utilize dashboards para visualizar o status da pipeline em tempo real.</li> </ul> <p>Alertas e Notifica\u00e7\u00f5es:</p> <ul> <li>Configure alertas via SNS (Simple Notification Service) ou outras ferramentas para receber notifica\u00e7\u00f5es imediatas sobre falhas ou erros.</li> </ul> <p>Logs e Auditoria:</p> <ul> <li>Analise os logs para identificar mensagens de erro, exce\u00e7\u00f5es e padr\u00f5es que possam indicar a origem do problema.</li> <li>Use o AWS CloudTrail para rastrear a\u00e7\u00f5es feitas na infraestrutura que possam ter causado falhas.</li> </ul> <p>Corre\u00e7\u00e3o de Falhas</p> <p>Diagn\u00f3stico:</p> <ul> <li>Identifique exatamente onde a falha ocorreu (ex: no processo de ingest\u00e3o, transforma\u00e7\u00e3o, carregamento, etc.).</li> <li>Verifique depend\u00eancias externas, como conex\u00f5es com bancos de dados, permiss\u00f5es e quotas.</li> </ul> <p>Resolu\u00e7\u00e3o:</p> <ul> <li>Se for um erro tempor\u00e1rio (ex: timeout, falha de rede), pode ser suficiente reiniciar o job ou a fun\u00e7\u00e3o.</li> <li>Corrija erros de configura\u00e7\u00e3o ou c\u00f3digo conforme identificado nos logs.</li> <li>Atualize permiss\u00f5es ou pol\u00edticas de acesso se necess\u00e1rio.</li> </ul> <p>Testes e Valida\u00e7\u00e3o:</p> <ul> <li>Execute testes localmente ou em ambiente de staging para garantir que o problema foi resolvido.</li> <li>Monitore a pipeline ap\u00f3s a corre\u00e7\u00e3o para garantir que a falha n\u00e3o se repita.</li> </ul> <p>Boas Pr\u00e1ticas</p> <ul> <li>Implementar retry logic e tratamento de erros dentro da pipeline.</li> <li>Usar versionamento para c\u00f3digo e configura\u00e7\u00f5es para facilitar rollback.</li> <li>Ter um plano de recupera\u00e7\u00e3o e comunica\u00e7\u00e3o claro para incidentes.</li> </ul>"},{"location":"02-consultas-sql/","title":"Consultas SQL","text":"<p>Esta se\u00e7\u00e3o tem como objetivo armazenar consultas SQL \u00fateis, servindo tanto como material de estudo quanto refer\u00eancia pr\u00e1tica para futuras necessidades.</p>"},{"location":"02-consultas-sql/#extrair-id-de-arn","title":"Extrair ID de ARN","text":"<p>Objetivo: Extrair os IDs dos recursos de API Gateway e nomes de fun\u00e7\u00f5es Lambda presentes na tabela fict\u00edtica <code>base_cloudability</code>, identificando e categorizando os registros conforme o tipo.</p> Clique para visualizar a consulta de cria\u00e7\u00e3o da tabela fict\u00edcia <pre><code>CREATE TABLE base_cloudability (\n    tipo_recurso VARCHAR(50),\n    recurso TEXT\n);\n\nINSERT INTO base_cloudability (tipo_recurso, recurso) VALUES\n('API Gateway', 'arn:aws:apigateway:us-east-1::/apis/aa11bb22cc/resources/abc123/method/GET'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/apis/dd33ee44ff/resources/def456/method/POST'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/gg55hh66ii/resources/ghi789/method/PUT'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/jj77kk88ll/resources/jkl012/method/DELETE'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/mm99nn00oo/resources/mno345/method/PATCH'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/pp11qq22rr/resources/pqr678/method/GET'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/ss33tt44uu/resources/stu901/method/POST'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/vv55ww66xx/resources/vwx234/method/PUT'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/yy77zz88aa/resources/yzb567/method/DELETE'),\n('API Gateway', 'arn:aws:apigateway:us-east-1::/restapis/bb99cc00dd/resources/bcd890/method/PATCH'),\n\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_aa11bb22cc/prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_dd33ee44ff/prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_gg55hh66ii/prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_jj77kk88ll/prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:API-Gateway-Execution-Logs_mm99nn00oo/prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:api-gateway-access-logs_pp11qq22rr_prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:api-gateway-access-logs_ss33tt44uu_prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:api-gateway-access-logs_vv55ww66xx_prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:api-gateway-access-logs_yy77zz88aa_prod'),\n('Log API Gateway', 'arn:aws:logs:us-east-1:123456789012:log-group:api-gateway-access-logs_bb99cc00dd_prod'),\n\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api1'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api2'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api3'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api4'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api5'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api6'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api7'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api8'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api9'),\n('Lambda Authorizer', 'arn:aws:lambda:us-east-1:123456789012:function:valida-token-api10'),\n\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api1'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api2'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api3'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api4'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api5'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api6'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api7'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api8'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api9'),\n('Log Lambda Authorizer', 'arn:aws:logs:us-east-1:123456789012:log-group:/aws/lambda/valida-token-api10');\n</code></pre>"},{"location":"02-consultas-sql/#consulta-no-postgresql-v17","title":"Consulta no PostgreSQL v17","text":"<pre><code>WITH api_ids AS (\n    SELECT\n        recurso,\n        COALESCE(\n            substring(recurso FROM '/restapis/([a-z0-9]{10})'),\n            substring(recurso FROM '/apis/([a-z0-9]{10})')\n        ) AS id_arn,\n        'API Gateway ID' AS tipo\n    FROM base_cloudability\n    WHERE recurso LIKE '%/restapis/%' OR recurso LIKE '%/apis/%'\n),\nlambda_names AS (\n    SELECT\n        recurso,\n        COALESCE(\n            substring(recurso FROM 'function:([^/]+)'),\n            substring(recurso FROM '/aws/lambda/([^/]+)')\n        ) AS id_arn,\n        'Lambda Function Name' AS tipo\n    FROM base_cloudability\n    WHERE recurso LIKE '%function:%' OR recurso LIKE '%/aws/lambda/%'\n)\nSELECT * FROM api_ids\nUNION ALL\nSELECT * FROM lambda_names\n</code></pre>"},{"location":"02-consultas-sql/#consulta-no-aws-athena","title":"Consulta no AWS Athena","text":"<pre><code>WITH api_ids AS (\n    SELECT\n        recurso,\n        COALESCE(\n            regexp_extract(recurso, '/restapis/([a-z0-9]{10})', 1),\n            regexp_extract(recurso, '/apis/([a-z0-9]{10})', 1)\n        ) AS id_arn,\n        'API Gateway ID' AS tipo\n    FROM base_cloudability\n    WHERE recurso LIKE '%/restapis/%' OR recurso LIKE '%/apis/%'\n),\nlambda_names AS (\n    SELECT\n        recurso,\n        COALESCE(\n            regexp_extract(recurso, 'function:([^/]+)', 1),\n            regexp_extract(recurso, '/aws/lambda/([^/]+)', 1)\n        ) AS id_arn,\n        'Lambda Function Name' AS tipo\n    FROM base_cloudability\n    WHERE recurso LIKE '%function:%' OR recurso LIKE '%/aws/lambda/%'\n)\nSELECT * FROM api_ids\nUNION ALL\nSELECT * FROM lambda_names\n</code></pre>"},{"location":"Projeto%20Real%20Time%20Analytics/01-projeto-real-time-analytics/","title":"Objetivos","text":""},{"location":"Projeto%20Real%20Time%20Analytics/01-projeto-real-time-analytics/#objetivo-geral","title":"Objetivo Geral","text":"<p>O objetivo desse projeto \u00e9 desenvolver um pipeline que consome dados de uma API, emite alertas em tempo real e armazena esses dados de maneira eficiente e organizada. Utilizando um conjunto de servi\u00e7os da AWS, ser\u00e1 poss\u00edvel integrar e orquestrar fun\u00e7\u00f5es lambda, Kinesis, SNS, roles IAM, S3, Glue (ETL, cat\u00e1logo e crawler), Athena e CloudWatch para criar uma solu\u00e7\u00e3o completa e escal\u00e1vel de an\u00e1lise de dados em tempo real.</p>"},{"location":"Projeto%20Real%20Time%20Analytics/01-projeto-real-time-analytics/#objetivos-especificos","title":"Objetivos Espec\u00edficos","text":"<ul> <li> <p>Configura\u00e7\u00e3o e integra\u00e7\u00e3o de fun\u00e7\u00f5es lambda e Kinesis para ingest\u00e3o e processamento de dados em tempo real.</p> </li> <li> <p>Uso de SNS para alertas em tempo real, garantindo que as informa\u00e7\u00f5es cr\u00edticas cheguem rapidamente aos destinat\u00e1rios corretos.</p> </li> <li> <p>Gerenciamento de roles IAM para garantir seguran\u00e7a e controle de acesso.</p> </li> <li> <p>Armazenamento de dados no S3 em duas \u00e1reas distintas: raw, para dados brutos, e gold, para dados transformados e estruturados em formato parquet.</p> </li> <li> <p>ETL com Glue, incluindo cria\u00e7\u00e3o de cat\u00e1logos e utiliza\u00e7\u00e3o de crawlers para organizar e transformar dados de maneira eficiente.</p> </li> <li> <p>Consultas ad-hoc com Athena para an\u00e1lise r\u00e1pida e flex\u00edvel dos dados armazenados.</p> </li> <li> <p>Monitoramento e logging com CloudWatch, assegurando que todo o processo seja monitorado e quaisquer problemas sejam rapidamente identificados e resolvidos.</p> </li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/","title":"Conceitos Importantes","text":"<p>Nesta se\u00e7\u00e3o, s\u00e3o apresentados os principais conceitos necess\u00e1rios para o desenvolvimento do projeto. Eles fornecem uma base te\u00f3rica para compreens\u00e3o dos m\u00e9todos e tecnologias utilizadas.</p>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#processamento-em-streaming","title":"Processamento em Streaming","text":"<ul> <li>Defini\u00e7\u00e3o: Processa dados em tempo real, \u00e0 medida que s\u00e3o gerados. </li> <li>Uso: Ideal para aplica\u00e7\u00f5es que exigem respostas r\u00e1pidas, como monitoramento de transa\u00e7\u00f5es financeiras, detec\u00e7\u00e3o de fraudes e an\u00e1lise em tempo real.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#processamento-em-batch","title":"Processamento em Batch","text":"<ul> <li>Defini\u00e7\u00e3o: Processa grandes volumes de dados em intervalos programados. </li> <li>Uso: Recomendado para tarefas que n\u00e3o exigem resposta imediata, como relat\u00f3rios financeiros mensais, an\u00e1lises hist\u00f3ricas e agrega\u00e7\u00e3o de grandes conjuntos de dados.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#quando-utilizar-cada-um","title":"Quando Utilizar Cada Um","text":"<ul> <li>Streaming: Quando a lat\u00eancia \u00e9 cr\u00edtica e os dados precisam ser processados em tempo real.</li> <li>Batch: Quando o volume de dados \u00e9 alto e o processamento em tempo real n\u00e3o \u00e9 necess\u00e1rio.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#produtores-consumidores-e-brokers","title":"Produtores, Consumidores e Brokers","text":"<ul> <li>Produtores: Entidades ou aplica\u00e7\u00f5es que geram e enviam dados para o sistema de streaminga (Sensores IoT, Aplica\u00e7\u00f5es Web).</li> <li>Consumidores: Entidades ou aplica\u00e7\u00f5es que recebem e processam os dados do sistema de streaming (Dashboards em Tempo Real, Sistemas de Alerta).</li> <li>Brokers: Sistemas intermedi\u00e1rios que recebem dados dos produtores e os distribuem aos consumidores (Apache Kafka, AWS Kinesis e RabbitMQ.).</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#tipos-de-janelas-em-aplicacoes-de-streaming","title":"Tipos de Janelas em Aplica\u00e7\u00f5es de Streaming","text":""},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#janela-de-tempo-time-window","title":"Janela de Tempo (Time Window)","text":"<ul> <li>Defini\u00e7\u00e3o: Agrupa eventos com base em intervalos de tempo fixos. </li> <li>Exemplo: Uma janela de 5 minutos coleta todos os eventos que ocorrem nesse per\u00edodo.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#janela-de-contagem-count-window","title":"Janela de Contagem (Count Window)","text":"<ul> <li>Defini\u00e7\u00e3o: Agrupa eventos com base em uma quantidade fixa de eventos. </li> <li>Exemplo: Uma janela de contagem de 100 eventos processa os primeiros 100 eventos e ent\u00e3o inicia uma nova janela.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#janela-tumbling-tumbling-window","title":"Janela Tumbling (Tumbling Window)","text":"<ul> <li>Defini\u00e7\u00e3o: Uma janela de tempo fixa e n\u00e3o sobreposta. Cada intervalo \u00e9 independente e n\u00e3o compartilha eventos com outras janelas.</li> <li>Exemplo: Uma janela tumbling de 10 segundos cria uma nova janela a cada 10 segundos, sem sobreposi\u00e7\u00e3o.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#tipos-de-garantia-de-entrega-em-streaming","title":"Tipos de Garantia de Entrega em Streaming","text":""},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#at-most-once-no-maximo-uma-vez","title":"At-Most-Once (No m\u00e1ximo uma vez)","text":"<ul> <li>Defini\u00e7\u00e3o: Uma mensagem pode ser entregue no m\u00e1ximo uma vez. Se ocorrer uma falha durante o envio, a mensagem pode ser perdida.</li> <li>Uso: Ideal para casos onde a perda de mensagens \u00e9 aceit\u00e1vel e a lat\u00eancia \u00e9 cr\u00edtica, como monitoramento de m\u00e9tricas n\u00e3o cr\u00edticas.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#at-least-once-no-minimo-uma-vez","title":"At-Least-Once (No m\u00ednimo uma vez)","text":"<ul> <li>Defini\u00e7\u00e3o: Garante que uma mensagem seja entregue pelo menos uma vez, mesmo que isso signifique entreg\u00e1-la mais de uma vez em casos de falha.</li> <li>Uso: Recomendado para cen\u00e1rios em que a perda de mensagens n\u00e3o \u00e9 aceit\u00e1vel, mesmo que ocorra duplicidade, como em sistemas de faturamento.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#exactly-once-exatamente-uma-vez","title":"Exactly-Once (Exatamente uma vez)","text":"<ul> <li>Defini\u00e7\u00e3o: Garante que cada mensagem seja entregue exatamente uma vez, sem duplicidade e sem perda.</li> <li>Uso: Essencial para aplica\u00e7\u00f5es cr\u00edticas onde a precis\u00e3o \u00e9 fundamental, como em processamento de pagamentos e transa\u00e7\u00f5es financeiras.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#estrategias-de-escalabilidade-em-sistemas-distribuidos","title":"Estrat\u00e9gias de Escalabilidade em Sistemas Distribu\u00eddos","text":""},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#partitioning-particionamento","title":"Partitioning (Particionamento)","text":"<ul> <li>Defini\u00e7\u00e3o: Divide os dados em diferentes parti\u00e7\u00f5es com base em uma chave de particionamento, permitindo que sejam distribu\u00eddos em diferentes n\u00f3s.</li> <li>Uso: Comum em bancos de dados distribu\u00eddos e sistemas de filas, como Kafka e DynamoDB.</li> <li>Vantagem: Distribui a carga de trabalho, permitindo processamento paralelo e maior desempenho.</li> <li>Desvantagem: A escolha da chave de particionamento \u00e9 cr\u00edtica. Uma chave mal escolhida pode gerar parti\u00e7\u00f5es desbalanceadas, causando gargalos.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#sharding-fragmentacao","title":"Sharding (Fragmenta\u00e7\u00e3o)","text":"<ul> <li>Defini\u00e7\u00e3o: Uma forma espec\u00edfica de particionamento, onde os dados s\u00e3o divididos em \"shards\" (fragmentos), cada um armazenado em um n\u00f3 separado.</li> <li>Uso: Comum em bancos de dados relacionais e NoSQL, como MongoDB e MySQL.</li> <li>Vantagem: Permite escalar horizontalmente o banco de dados, mantendo o desempenho.</li> <li>Desvantagem: A complexidade na gest\u00e3o dos shards aumenta, especialmente na reconfigura\u00e7\u00e3o e migra\u00e7\u00e3o de dados entre shards.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#replicacao","title":"Replica\u00e7\u00e3o","text":"<ul> <li>Defini\u00e7\u00e3o: Mant\u00e9m m\u00faltiplas c\u00f3pias dos dados em diferentes n\u00f3s, garantindo alta disponibilidade e toler\u00e2ncia a falhas.</li> <li>Uso: Amplamente utilizado em bancos de dados distribu\u00eddos, como PostgreSQL e Cassandra.</li> <li>Vantagem: Melhora a disponibilidade e a resili\u00eancia do sistema, permitindo recupera\u00e7\u00e3o em caso de falhas.</li> <li>Desvantagem: Aumenta o consumo de armazenamento e o custo de rede. Sincronizar m\u00faltiplas c\u00f3pias pode ser complexo.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#auto-scaling","title":"Auto Scaling","text":"<ul> <li>Defini\u00e7\u00e3o: Ajusta automaticamente os recursos de acordo com a demanda, aumentando ou diminuindo o n\u00famero de inst\u00e2ncias ou capacidade de acordo com o uso.</li> <li>Uso: Comum em plataformas de nuvem, como AWS Auto Scaling, Kubernetes HPA (Horizontal Pod Autoscaler).</li> <li>Vantagem: Garante que os recursos sejam utilizados de forma eficiente, evitando custos desnecess\u00e1rios e mantendo o desempenho.</li> <li>Desvantagem: Pode gerar custos inesperados se n\u00e3o estiver configurado corretamente. Depende de m\u00e9tricas precisas para funcionar de forma eficaz.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#estrategias-de-tolerancia-a-falhas-em-sistemas-distribuidos","title":"Estrat\u00e9gias de Toler\u00e2ncia a Falhas em Sistemas Distribu\u00eddos","text":""},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#replicacao-de-dados","title":"Replica\u00e7\u00e3o de Dados","text":"<ul> <li>Defini\u00e7\u00e3o: Mant\u00e9m m\u00faltiplas c\u00f3pias dos dados em diferentes n\u00f3s ou regi\u00f5es, garantindo que estejam dispon\u00edveis mesmo em caso de falha de um dos n\u00f3s.</li> <li>Uso: Comum em bancos de dados distribu\u00eddos como Cassandra, MongoDB e sistemas de armazenamento em nuvem como Amazon S3.</li> <li>Vantagem: Melhora a disponibilidade e a resili\u00eancia dos dados.</li> <li>Desvantagem: Aumenta o custo de armazenamento e pode gerar inconsist\u00eancias em sistemas que n\u00e3o possuem replica\u00e7\u00e3o s\u00edncrona.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#checkpoints","title":"Checkpoints","text":"<ul> <li>Defini\u00e7\u00e3o: Armazena periodicamente o estado de um sistema, permitindo que ele seja restaurado a partir desse ponto em caso de falha.</li> <li>Uso: Comum em sistemas de processamento em streaming, como Apache Flink e Spark Streaming.</li> <li>Vantagem: Minimiza a perda de dados e o tempo de recupera\u00e7\u00e3o ap\u00f3s uma falha.</li> <li>Desvantagem: Pode consumir recursos adicionais de armazenamento e processamento. A frequ\u00eancia dos checkpoints afeta o desempenho.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#failover-automatico","title":"Failover Autom\u00e1tico","text":"<ul> <li>Defini\u00e7\u00e3o: Redireciona automaticamente o tr\u00e1fego ou a carga de trabalho para uma inst\u00e2ncia de backup em caso de falha da inst\u00e2ncia prim\u00e1ria.</li> <li>Uso: Comum em servidores de aplica\u00e7\u00e3o, bancos de dados distribu\u00eddos e sistemas de balanceamento de carga.</li> <li>Vantagem: Garante alta disponibilidade e continuidade do servi\u00e7o sem interven\u00e7\u00e3o manual.</li> <li>Desvantagem: A configura\u00e7\u00e3o inadequada pode resultar em tempo de inatividade. Requer monitoramento constante e infraestrutura adicional.</li> </ul>"},{"location":"Projeto%20Real%20Time%20Analytics/02-conceitos-importantes/#processamento-idempotente","title":"Processamento Idempotente","text":"<ul> <li>Defini\u00e7\u00e3o: Garante que uma opera\u00e7\u00e3o pode ser aplicada v\u00e1rias vezes sem alterar o resultado final, evitando duplicidade de processamento.</li> <li>Uso: Comum em sistemas de APIs, processamento de mensagens (como Kafka) e transa\u00e7\u00f5es financeiras.</li> <li>Vantagem: Evita efeitos indesejados em caso de reenvio ou reprocessamento de mensagens.</li> <li>Desvantagem: Requer design cuidadoso das opera\u00e7\u00f5es para garantir idempot\u00eancia. Pode aumentar a complexidade do desenvolvimento.</li> </ul>"}]}